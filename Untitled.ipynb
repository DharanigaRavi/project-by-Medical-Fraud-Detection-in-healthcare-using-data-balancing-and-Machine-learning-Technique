{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba81286-6e05-4744-93c1-62caecacb3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cost_feature_1  util_feature_2  usage_score  billing_variance  \\\n",
      "0       -2.591459        1.264459     0.577366         -2.001795   \n",
      "1       -0.120761       -0.725202    -0.673109          0.361122   \n",
      "2        1.052110       -1.050024    -1.097380         -0.235955   \n",
      "3       -0.538686       -2.239094    -1.158789         -1.860463   \n",
      "4       -1.163512        0.247373     0.139034         -1.479911   \n",
      "\n",
      "   frequency_of_visits  anomaly_score  patient_age gender  provider_id  \\\n",
      "0             1.232588      -0.665552           69   Male         1264   \n",
      "1            -0.587576       1.496148           32  Other         4093   \n",
      "2             0.828609      -2.407889           89  Other         5798   \n",
      "3            -1.237884       0.265186           78   Male         9143   \n",
      "4             0.596953      -1.028998           38   Male         1325   \n",
      "\n",
      "   hospital_id  claim_amount  num_procedures  hospital_stay_days  \\\n",
      "0          171       9066.21               4                   2   \n",
      "1          292      12194.64               1                   9   \n",
      "2          421       6464.79               4                   9   \n",
      "3          210       8823.98               2                  16   \n",
      "4          392       4975.11               4                   8   \n",
      "\n",
      "  primary_diagnosis procedure_type  claim_day  claim_month  is_fraud  \n",
      "0          Migraine      Emergency     Friday            3         0  \n",
      "1          Migraine        Surgery     Monday           10         1  \n",
      "2               Flu        Surgery   Thursday           12         0  \n",
      "3            Cancer      Radiology  Wednesday            1         1  \n",
      "4          Fracture        Therapy     Monday           10         0  \n",
      "‚úÖ Dataset saved as 'enhanced_medical_fraud_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "n_samples = 10000\n",
    "fraud_ratio = 0.1\n",
    "\n",
    "# Generate core features and fraud labels\n",
    "X, y = make_classification(n_samples=n_samples, \n",
    "                           n_features=6, \n",
    "                           n_informative=4, \n",
    "                           weights=[1 - fraud_ratio, fraud_ratio], \n",
    "                           flip_y=0.01, \n",
    "                           random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['cost_feature_1', 'util_feature_2', 'usage_score', 'billing_variance',\n",
    "                              'frequency_of_visits', 'anomaly_score'])\n",
    "\n",
    "# Add realistic features\n",
    "np.random.seed(42)\n",
    "\n",
    "# Patient and Provider Info\n",
    "df['patient_age'] = np.random.randint(18, 90, size=n_samples)\n",
    "df['gender'] = np.random.choice(['Male', 'Female', 'Other'], size=n_samples)\n",
    "df['provider_id'] = np.random.randint(1000, 9999, size=n_samples)\n",
    "df['hospital_id'] = np.random.randint(100, 500, size=n_samples)\n",
    "\n",
    "# Claim Info\n",
    "df['claim_amount'] = np.abs(np.random.normal(loc=8000, scale=4000, size=n_samples)).round(2)\n",
    "df['num_procedures'] = np.random.poisson(lam=3, size=n_samples)\n",
    "df['hospital_stay_days'] = np.random.randint(1, 20, size=n_samples)\n",
    "df['primary_diagnosis'] = np.random.choice(['Diabetes', 'Heart Disease', 'Fracture', 'Cancer', 'Flu', 'Migraine'], size=n_samples)\n",
    "df['procedure_type'] = np.random.choice(['Surgery', 'Therapy', 'Radiology', 'Consultation', 'Emergency'], size=n_samples)\n",
    "\n",
    "# Temporal Info\n",
    "df['claim_day'] = np.random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'], size=n_samples)\n",
    "df['claim_month'] = np.random.choice(range(1, 13), size=n_samples)\n",
    "\n",
    "# Target Label\n",
    "df['is_fraud'] = y\n",
    "\n",
    "# Preview and save\n",
    "print(df.head())\n",
    "df.to_csv('enhanced_medical_fraud034         -1.479911   \n",
    "\n",
    "   frequency_of_visits  anomaly_score  patient_age gender  provider_id  \\\n",
    "0             1.232588      -0.665552           69   Male         1264   \n",
    "1            -0.587576       1.496148           32  Other         4093   \n",
    "2             0.828609      -2.407889           89  Other         5798   \n",
    "3            -1.237884       0.265186           78   Male         9143   \n",
    "4             0.596953      -1.028998           38   Male         1325   \n",
    "\n",
    "   hospital_id  claim_amount  num_procedures  hospital_stay_days  \\\n",
    "0          171       9066.21               4                   2   _dataset.csv', index=False)\n",
    "print(\"‚úÖ Dataset saved as 'enhanced_medical_fraud_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704f0fe8-9fde-4c43-8186-5191ae036d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and label encoders saved successfully.\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      2685\n",
      "           1       0.70      0.76      0.73       315\n",
      "\n",
      "    accuracy                           0.94      3000\n",
      "   macro avg       0.83      0.86      0.85      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:19:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"enhanced_medical_fraud_dataset.csv\")\n",
    "\n",
    "# Step 1: Label Encoding for Categorical Variables\n",
    "categorical_cols = ['gender', 'primary_diagnosis', 'procedure_type', 'claim_day']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # store encoder for future use\n",
    "\n",
    "# Step 2: Features and Labels\n",
    "X = df.drop(\"is_fraud\", axis=1)\n",
    "y = df[\"is_fraud\"]\n",
    "\n",
    "# Step 3: Split and balance with SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 4: Train model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 5: Save model and encoders to pickle\n",
    "with open(\"xgboost_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "with open(\"label_encoders.pkl\", \"wb\") as le_file:\n",
    "    pickle.dump(label_encoders, le_file)\n",
    "\n",
    "print(\"‚úÖ Model and label encoders saved successfully.\")\n",
    "\n",
    "# Optional: Test model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"üìä Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85faa718-7f61-462f-810d-c18a802921cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loaded Data:\n",
      "   cost_feature_1  util_feature_2  usage_score  billing_variance  \\\n",
      "0       -2.591459        1.264459     0.577366         -2.001795   \n",
      "1       -0.120761       -0.725202    -0.673109          0.361122   \n",
      "2        1.052110       -1.050024    -1.097380         -0.235955   \n",
      "3       -0.538686       -2.239094    -1.158789         -1.860463   \n",
      "4       -1.163512        0.247373     0.139034         -1.479911   \n",
      "\n",
      "   frequency_of_visits  anomaly_score  patient_age gender  provider_id  \\\n",
      "0             1.232588      -0.665552           69   Male         1264   \n",
      "1            -0.587576       1.496148           32  Other         4093   \n",
      "2             0.828609      -2.407889           89  Other         5798   \n",
      "3            -1.237884       0.265186           78   Male         9143   \n",
      "4             0.596953      -1.028998           38   Male         1325   \n",
      "\n",
      "   hospital_id  claim_amount  num_procedures  hospital_stay_days  \\\n",
      "0          171       9066.21               4                   2   \n",
      "1          292      12194.64               1                   9   \n",
      "2          421       6464.79               4                   9   \n",
      "3          210       8823.98               2                  16   \n",
      "4          392       4975.11               4                   8   \n",
      "\n",
      "  primary_diagnosis procedure_type  claim_day  claim_month  is_fraud  \n",
      "0          Migraine      Emergency     Friday            3         0  \n",
      "1          Migraine        Surgery     Monday           10         1  \n",
      "2               Flu        Surgery   Thursday           12         0  \n",
      "3            Cancer      Radiology  Wednesday            1         1  \n",
      "4          Fracture        Therapy     Monday           10         0  \n",
      "\n",
      "üî§ Encoding Categorical Features:\n",
      " - gender classes: ['Female', 'Male', 'Other']\n",
      " - primary_diagnosis classes: ['Cancer', 'Diabetes', 'Flu', 'Fracture', 'Heart Disease', 'Migraine']\n",
      " - procedure_type classes: ['Consultation', 'Emergency', 'Radiology', 'Surgery', 'Therapy']\n",
      " - claim_day classes: ['Friday', 'Monday', 'Thursday', 'Tuesday', 'Wednesday']\n",
      "\n",
      "üìä Feature Matrix Shape: (10000, 17)\n",
      "üéØ Target Distribution:\n",
      " is_fraud\n",
      "0    8951\n",
      "1    1049\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üß™ Training set size: (7000, 17)\n",
      "üß™ Test set size: (3000, 17)\n",
      "\n",
      "‚öñÔ∏è After SMOTE Balancing:\n",
      " - X_train_bal shape: (12532, 17)\n",
      " - Class distribution:\n",
      " is_fraud\n",
      "0    6266\n",
      "1    6266\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üß† Model Trained Successfully\n",
      "\n",
      "üìà Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      2685\n",
      "           1       0.70      0.76      0.73       315\n",
      "\n",
      "    accuracy                           0.94      3000\n",
      "   macro avg       0.83      0.86      0.85      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n",
      "üßÆ Confusion Matrix:\n",
      "[[2580  105]\n",
      " [  75  240]]\n",
      "\n",
      "‚úÖ XGBoost model saved as 'xgboost_model.pkl'\n",
      "‚úÖ Label encoders saved as 'label_encoders.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:20:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "df = pd.read_csv(\"enhanced_medical_fraud_dataset.csv\")\n",
    "print(\"üì• Loaded Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Label Encoding\n",
    "categorical_cols = ['gender', 'primary_diagnosis', 'procedure_type', 'claim_day']\n",
    "label_encoders = {}\n",
    "\n",
    "print(\"\\nüî§ Encoding Categorical Features:\")\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\" - {col} classes: {list(le.classes_)}\")\n",
    "\n",
    "# Step 3: Split Features and Target\n",
    "X = df.drop(\"is_fraud\", axis=1)\n",
    "y = df[\"is_fraud\"]\n",
    "print(\"\\nüìä Feature Matrix Shape:\", X.shape)\n",
    "print(\"üéØ Target Distribution:\\n\", y.value_counts())\n",
    "\n",
    "# Step 4: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "print(\"\\nüß™ Training set size:\", X_train.shape)\n",
    "print(\"üß™ Test set size:\", X_test.shape)\n",
    "\n",
    "# Step 5: Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚öñÔ∏è After SMOTE Balancing:\")\n",
    "print(\" - X_train_bal shape:\", X_train_bal.shape)\n",
    "print(\" - Class distribution:\\n\", pd.Series(y_train_bal).value_counts())\n",
    "\n",
    "# Step 6: Train XGBoost\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "print(\"\\nüß† Model Trained Successfully\")\n",
    "\n",
    "# Step 7: Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"üßÆ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Step 8: Save Model and Encoders\n",
    "with open(\"xgboost_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "print(\"\\n‚úÖ XGBoost model saved as 'xgboost_model.pkl'\")\n",
    "\n",
    "with open(\"label_encoders.pkl\", \"wb\") as le_file:\n",
    "    pickle.dump(label_encoders, le_file)\n",
    "print(\"‚úÖ Label encoders saved as 'label_encoders.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d43a48-f516-4fc5-aea4-8e54637295c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä New Data (Before Encoding):\n",
      "  gender primary_diagnosis procedure_type claim_day  patient_age  \\\n",
      "0   Male            Cancer        Surgery    Monday           45   \n",
      "\n",
      "   claim_amount  num_procedures  hospital_stay_days  cost_feature_1  \\\n",
      "0         12000               3                   7             0.3   \n",
      "\n",
      "   util_feature_2  usage_score  billing_variance  frequency_of_visits  \\\n",
      "0             0.2          0.5               0.1                  0.5   \n",
      "\n",
      "   anomaly_score  \n",
      "0            0.3  \n",
      "\n",
      "üî§ New Data (After Encoding):\n",
      "   gender  primary_diagnosis  procedure_type  claim_day  patient_age  \\\n",
      "0       1                  0               3          1           45   \n",
      "\n",
      "   claim_amount  num_procedures  hospital_stay_days  cost_feature_1  \\\n",
      "0         12000               3                   7             0.3   \n",
      "\n",
      "   util_feature_2  usage_score  billing_variance  frequency_of_visits  \\\n",
      "0             0.2          0.5               0.1                  0.5   \n",
      "\n",
      "   anomaly_score  \n",
      "0            0.3  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['cost_feature_1', 'util_feature_2', 'usage_score', 'billing_variance', 'frequency_of_visits', 'anomaly_score', 'patient_age', 'gender', 'provider_id', 'hospital_id', 'claim_amount', 'num_procedures', 'hospital_stay_days', 'primary_diagnosis', 'procedure_type', 'claim_day', 'claim_month'] ['gender', 'primary_diagnosis', 'procedure_type', 'claim_day', 'patient_age', 'claim_amount', 'num_procedures', 'hospital_stay_days', 'cost_feature_1', 'util_feature_2', 'usage_score', 'billing_variance', 'frequency_of_visits', 'anomaly_score']\nexpected hospital_id, provider_id, claim_month in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Step 4: Make prediction\u001b[39;00m\n\u001b[32m     40\u001b[39m X_new = new_data  \u001b[38;5;66;03m# Features for prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Step 5: Display the prediction result\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí° Predictions:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1718\u001b[39m, in \u001b[36mXGBClassifier.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1707\u001b[39m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m   1709\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1715\u001b[39m     iteration_range: Optional[IterationRange] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1716\u001b[39m ) -> ArrayLike:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity=\u001b[38;5;28mself\u001b[39m.verbosity):\n\u001b[32m-> \u001b[39m\u001b[32m1718\u001b[39m         class_probs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1719\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1725\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[32m   1726\u001b[39m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[32m   1727\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2667\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2665\u001b[39m     data, fns, _ = _transform_pandas_df(data, enable_categorical)\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[32m-> \u001b[39m\u001b[32m2667\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[32m   2669\u001b[39m     data = np.array(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:3243\u001b[39m, in \u001b[36mBooster._validate_features\u001b[39m\u001b[34m(self, feature_names)\u001b[39m\n\u001b[32m   3237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[32m   3238\u001b[39m     msg += (\n\u001b[32m   3239\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mtraining data did not have the following fields: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3240\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[32m   3241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3243\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg.format(\u001b[38;5;28mself\u001b[39m.feature_names, feature_names))\n",
      "\u001b[31mValueError\u001b[39m: feature_names mismatch: ['cost_feature_1', 'util_feature_2', 'usage_score', 'billing_variance', 'frequency_of_visits', 'anomaly_score', 'patient_age', 'gender', 'provider_id', 'hospital_id', 'claim_amount', 'num_procedures', 'hospital_stay_days', 'primary_diagnosis', 'procedure_type', 'claim_day', 'claim_month'] ['gender', 'primary_diagnosis', 'procedure_type', 'claim_day', 'patient_age', 'claim_amount', 'num_procedures', 'hospital_stay_days', 'cost_feature_1', 'util_feature_2', 'usage_score', 'billing_variance', 'frequency_of_visits', 'anomaly_score']\nexpected hospital_id, provider_id, claim_month in input data"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load the trained model and label encoders\n",
    "with open(\"xgboost_model.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "with open(\"label_encoders.pkl\", \"rb\") as le_file:\n",
    "    label_encoders = pickle.load(le_file)\n",
    "\n",
    "# Step 2: Prepare new data for prediction\n",
    "# Let's assume new_data is a DataFrame for which we want predictions\n",
    "new_data = pd.DataFrame({\n",
    "    'gender': ['Male'], \n",
    "    'primary_diagnosis': ['Cancer'], \n",
    "    'procedure_type': ['Surgery'],\n",
    "    'claim_day': ['Monday'],\n",
    "    'patient_age': [45], \n",
    "    'claim_amount': [12000],\n",
    "    'num_procedures': [3], \n",
    "    'hospital_stay_days': [7],\n",
    "    'cost_feature_1': [0.3], 'util_feature_2': [0.2], 'usage_score': [0.5],\n",
    "    'billing_variance': [0.1], 'frequency_of_visits': [0.5], 'anomaly_score': [0.3]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä New Data (Before Encoding):\")\n",
    "print(new_data)\n",
    "\n",
    "# Step 3: Apply label encoding to categorical columns\n",
    "categorical_cols = ['gender', 'primary_diagnosis', 'procedure_type', 'claim_day']\n",
    "for col in categorical_cols:\n",
    "    le = label_encoders[col]\n",
    "    new_data[col] = le.transform(new_data[col])\n",
    "\n",
    "print(\"\\nüî§ New Data (After Encoding):\")\n",
    "print(new_data)\n",
    "\n",
    "# Step 4: Make prediction\n",
    "X_new = new_data  # Features for prediction\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Step 5: Display the prediction result\n",
    "print(\"\\nüí° Predictions:\")\n",
    "print(\" - Fraud (1) or Legit (0):\", predictions)\n",
    "\n",
    "# If you want the prediction to be in readable form\n",
    "if predictions[0] == 1:\n",
    "    print(\"‚ö†Ô∏è Fraud detected!\")\n",
    "else:\n",
    "    print(\"‚úÖ No fraud detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f138cd5-9fdc-443a-99bd-633bcc733037",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'gender': ['Male'], \n",
    "    'primary_diagnosis': ['Cancer'], \n",
    "    'procedure_type': ['Surgery'],\n",
    "    'claim_day': ['Monday'],\n",
    "    'patient_age': [45], \n",
    "    'claim_amount': [12000],\n",
    "    'num_procedures': [3], \n",
    "    'hospital_stay_days': [7],\n",
    "    'cost_feature_1': [0.3], 'util_feature_2': [0.2], 'usage_score': [0.5],\n",
    "    'billing_variance': [0.1], 'frequency_of_visits': [0.5], 'anomaly_score': [0.3],\n",
    "    \n",
    "    # These columns were missing earlier but are essential\n",
    "    'hospital_id': [200],\n",
    "    'provider_id': [4000],\n",
    "    'claim_month': [5]  # Add month of the claim\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b25323-1769-48cd-ab3a-74d641d471a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic medical fraud dataset generated and saved as 'health_insurance_synthetic_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Define features and their random ranges\n",
    "data = {\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'primary_diagnosis': np.random.choice(['Cancer', 'Heart Disease', 'Diabetes', 'Other'], n_samples),\n",
    "    'procedure_type': np.random.choice(['Surgery', 'Consultation', 'Test', 'Emergency'], n_samples),\n",
    "    'claim_day': np.random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], n_samples),\n",
    "    'patient_age': np.random.randint(18, 80, n_samples),\n",
    "    'claim_amount': np.random.uniform(1000, 20000, n_samples),\n",
    "    'num_procedures': np.random.randint(1, 6, n_samples),\n",
    "    'hospital_stay_days': np.random.randint(1, 14, n_samples),\n",
    "    'cost_feature_1': np.random.uniform(0, 1, n_samples),\n",
    "    'util_feature_2': np.random.uniform(0, 1, n_samples),\n",
    "    'usage_score': np.random.uniform(0, 1, n_samples),\n",
    "    'billing_variance': np.random.uniform(0, 1, n_samples),\n",
    "    'frequency_of_visits': np.random.uniform(0, 1, n_samples),\n",
    "    'anomaly_score': np.random.uniform(0, 1, n_samples),\n",
    "    'hospital_id': np.random.randint(1, 10, n_samples),\n",
    "    'provider_id': np.random.randint(1, 10, n_samples),\n",
    "    'claim_month': np.random.randint(1, 13, n_samples),\n",
    "    'fraud': np.random.choice([0, 1], n_samples, p=[0.85, 0.15])  # 85% Non-Fraud, 15% Fraud\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])\n",
    "df['primary_diagnosis'] = label_encoder.fit_transform(df['primary_diagnosis'])\n",
    "df['procedure_type'] = label_encoder.fit_transform(df['procedure_type'])\n",
    "df['claim_day'] = label_encoder.fit_transform(df['claim_day'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"health_insurance_synthetic_data.csv\", index=False)\n",
    "\n",
    "print(\"Synthetic medical fraud dataset generated and saved as 'health_insurance_synthetic_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adba29-6939-4f29-8019-d9d06f773b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
